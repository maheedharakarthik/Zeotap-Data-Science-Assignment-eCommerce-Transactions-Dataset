# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import davies_bouldin_score

# Loading datasets
customers = pd.read_csv('Customers.csv')
products = pd.read_csv('Products.csv')
transactions = pd.read_csv('Transactions.csv')

# Task 1: Exploratory Data Analysis (EDA) and Business Insights
# Data overview
print(customers.info())
print(products.info())
print(transactions.info())

# Merging datasets
data = pd.merge(transactions, customers, on='CustomerID')
data = pd.merge(data, products, on='ProductID')

# Generating basic EDA
print(data.describe())
print(data.isnull().sum())

# Visualizations
plt.figure(figsize=(10, 6))
sns.countplot(data['Region'])
plt.title('Customer Distribution by Region')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x='Category', y='TotalValue', data=data.groupby('Category').sum().reset_index())
plt.title('Revenue by Product Category')
plt.show()

# Task 2: Lookalike Model
# Aggregating customer features
if 'Price' not in data.columns:
    data['Price'] = data['TotalValue'] / data['Quantity']

customer_features = data.groupby('CustomerID').agg({
    'Quantity': 'sum',
    'TotalValue': 'sum',
    'Price': 'mean'
}).reset_index()

# Standardizing data
scaler = StandardScaler()
scaled_features = scaler.fit_transform(customer_features.iloc[:, 1:])

# Calculating similarity
similarity_matrix = cosine_similarity(scaled_features)
similarity_df = pd.DataFrame(similarity_matrix, index=customer_features['CustomerID'], columns=customer_features['CustomerID'])

# Creating Lookalike recommendations
lookalike_dict = {}
for customer in similarity_df.index[:20]:  # First 20 customers
    similar_customers = similarity_df[customer].sort_values(ascending=False).iloc[1:4]
    lookalike_dict[customer] = similar_customers.to_dict()

lookalike_df = pd.DataFrame([{"CustomerID": k, "SimilarCustomers": v} for k, v in lookalike_dict.items()])
lookalike_df.to_csv('Lookalike.csv', index=False)

# Task 3: Customer Segmentation / Clustering
# Preparing data for clustering
features = data.groupby('CustomerID').agg({
    'Quantity': 'sum',
    'TotalValue': 'sum',
    'Price': 'mean'
}).reset_index()
features_scaled = scaler.fit_transform(features.iloc[:, 1:])

# Applying KMeans
kmeans = KMeans(n_clusters=4, random_state=42)
features['Cluster'] = kmeans.fit_predict(features_scaled)

# Calculating Davies-Bouldin Index
db_index = davies_bouldin_score(features_scaled, features['Cluster'])
print(f"Davies-Bouldin Index: {db_index}")

# Visualization
plt.figure(figsize=(10, 6))
sns.scatterplot(x=features_scaled[:, 0], y=features_scaled[:, 1], hue=features['Cluster'], palette='viridis')
plt.title('Customer Segmentation')
plt.show()

# Save results
features.to_csv('Clustering_Results.csv', index=False)
